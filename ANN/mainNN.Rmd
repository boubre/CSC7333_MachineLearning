---
title: "Spam Detection using Neural Network"
author: "Ali & Zahra"
date: "Monday, April 27, 2015"
output: html_document
---
Let's write the function neuralnetwork

```{r,echo=FALSE}
neuralnetwork <- function(train.sample.in,train.sample.out,valid.sample.in,valid.sample.out,h){
#Using Neural Network:
# The package used is "neuralnet", which uses "resilient backpropagation" 
# method to train the neural network's structure.
library('neuralnet')

# For neural network input arguments, we should combine 
# both trainX and trainY together

train<-cbind(train.sample.in,train.sample.out)
colnames(train)<-c("V1","V2","V3","V4","V5","V6","V7","V8",
"V9","V10","V11","V12","V13","V14","V15","V16","V17","V18","V19","V20",
"V21","V22","V23","V24","V25","V26","V27","V28","V29","V30","V31","V32",
"V33","V34","V35","V36","V37","V38","V39","V40","V41","V42","V43","V44",
"V45","V46","V47","V48","V49","V50","V51","V52","V53","V54","V55","V56",
"V57","V58")

train.sample.out<-factor(train.sample.out)

# Now using "neuralnet" function, we can obtain the Neural network for our
# data. Since, thereare 57 different columns in the input matrix
# hence we only use the first 9 attributes to obtain the NN.
# Also, our NN has two hidden layers, each having 5 nodes
# the "threshold" value shows the  the threshold for the partial 
# derivatives of the error function as stopping criteria.
# using rprop+
# maximum training steps 20000
network<-neuralnet(V58~V7 + V53 + V24 + V23 + V16 + V25 + V21 + V26 + V52 + V27,
                   train,hidden=h,threshold=0.5,algorithm='rprop+',stepmax=20000)

# Let's plot the neural network: The Error part in the plot shows the
# MSE between trainY, which is the output for training dataset,
# and the output for NN
#jpeg(file="D://LSU//Spring 2015//Machine Learning//Project//R//spamNN.jpeg",
#width = 12, height = 17, unit="in",res=200)
#plot(network,file="spamNN.jpeg")
#dev.off()

#plot(network)

rows <- c(7,53,24,23,16,25,21,26,52,27)
# In this part we compute the MSE of NN regarding to training subset j
# we compute the NN output, using the attributes V1 to V9 in trainX
train_result<-compute(network,valid.sample.in[,rows])$net.result

# Then we round the real outputs of NN, into binary output 0 or 1
# Basically we "clip" the vector train_result, using "pmin" and "pmax"
# Functions, by limiting all the values to be either 0 or 1
# In future, we should refine this function, and use some other proper methods
binary_train_result<-pmin(pmax(round(train_result),0),1)
#train_output<-Y[1:800]

# Finally, we find the sum of all cases, where output of NN and true output are
# different. Also, we compute train_error, which is basically the MSE
# of NN for training data set, and for our proposed "threshold" function
#print(sum(abs(train_output-binary_train_result)))
err<-sum((valid.sample.out-binary_train_result)^2)/2
#print(train_error)

# The same procedure goes for the test data
#test_result<-compute(network,testX[,1:9])$net.result
#binary_test_result<-pmin(pmax(round(test_result),0),1)
#true_test_value<-Y[801:1200]
#print(sum(abs(binary_test_result-true_test_value)))
#test_error<-sum((binary_test_result-true_test_value)^2)/2
#print(test_error)
return(err)
}
```


First, let's read the spamtable.csv and define the input and output:
In particular, we assign 2000 samples for training data, 1000 samples for validation (if necessary), and the rest (1601 samples) are used for testing purposes.

```{r,echo=FALSE}
spam_data<-read.csv(
"D:/LSU/Spring 2015/Machine Learning/Project/R/spamtable.csv")
input<-spam_data[,1:57]
output<-spam_data[,58]
######################################
#####TRAINING SET#####################
#train.input<-input[1:2000,]
#train.output<-output[1:2000]
train.input<-input[1:4600,]
train.output<-output[1:4600]
#######################################
#####VALIDATION SET###################
valid.input <- input[2001:3000,]
valid.output <- output[2001:3000]
####################################
#####TEST SET#########################
test.input <- input[3001:4601,]
test.output <-output[3001:4601]
```

Now, we want to perform a k-fold cross validatation, by dividing the training set into k subsets, and finding the optimal number of hidden nodes (h) to put in neural network

```{r,echo=FALSE}
#####ASSIGNING VALUES TO k and h #####################
k <- 10
h <- c(1:10)
L <- nrow(train.input) # number of samples in training set
err <- c(1:k) # initialize
mean.err <- c(1:h) # initialize
#######################################################
####### Cross Validation #######################
for (i in h){
  for (j in 1:k){
    valid.sample.in <- train.input[((j-1)*L/k + 1):(j*L/k),] # validation subset j, for input
    valid.sample.out <- train.output[((j-1)*L/k + 1):(j*L/k)] # validation subset j, for output
    train.sample.in <- train.input[-(((j-1)*L/k + 1):(j*L/k)),] # training subset j, for input
    train.sample.out <- train.output[-(((j-1)*L/k + 1):(j*L/k))] # training subset j, for output
    ############################################
    ####### Finding ERROR #################
    err[j] <- neuralnetwork(train.sample.in,train.sample.out,valid.sample.in,valid.sample.out,i) # Finding the error for each one of k samples
  }
  #################################
  ############## Finding MEAN ERROR for EACH h #################
  mean.err[i] <- sum(err)/k
}
```


Now, Let's pick the h, that has minimum mean error

```{r}
### FINDING THE INDEX OF MIN VALUE ######
index <- which(mean.err == min(mean.err), arr.ind = TRUE)
##############################
#### IF WE HAVE MORE THAN ONE MIN VALUE, CHOOSE THE SMALLER ONE ######
best.h <- mean.err[index[1]]
print(mean.err)
print(best.h)
##################################
###### BEST ERROR PERCENTAGE###################
print(best.h/length(valid.sample.out)*100)
```


Now we should train the optimal Neural Network on the whole data set and compute the total error on this Network.


```{r,echo=FALSE}

train_final<-cbind(train.input,train.output)
colnames(train_final)<-c("V1","V2","V3","V4","V5","V6","V7","V8",
"V9","V10","V11","V12","V13","V14","V15","V16","V17","V18","V19","V20",
"V21","V22","V23","V24","V25","V26","V27","V28","V29","V30","V31","V32",
"V33","V34","V35","V36","V37","V38","V39","V40","V41","V42","V43","V44",
"V45","V46","V47","V48","V49","V50","V51","V52","V53","V54","V55","V56",
"V57","V58")

#train.output<-factor(train.output)

spam.neural.net<-neuralnet(V58~V7 + V53 + V24 + V23 + V16 + V25 + V21 + V26 + V52 + V27,
                   train_final,hidden=h[index[1]],threshold=0.5,learningrate=0.05,algorithm='backprop',stepmax=20000)

###########################
#### PLOTTING THE NETWORK #####################
plot(spam.neural.net)

rows <- c(7,53,24,23,16,25,21,26,52,27)
final_result<-compute(spam.neural.net,train_final[,rows])$net.result

binary_final_result<-pmin(pmax(round(final_result),0),1)
final_err<-sum((train.output-binary_final_result)^2)
final_err_percent<-final_err/nrow(train_final)*100
print(final_err_percent)





```


Let's see the number of false positives in this situation:

```{r}
# This shows a table for all possible combinations of TRUE and PREDICTED values
table(Truth = train.output, Prediction = binary_final_result)
```



